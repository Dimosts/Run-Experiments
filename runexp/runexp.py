import logging
import multiprocessing
import os
import copy
import platform
import sys
import json
import pickle
import traceback
import time

# Try importing resource, but don't fail if not available
try:
    import resource
    HAS_RESOURCE = True
except ImportError:
    HAS_RESOURCE = False

from tqdm.auto import tqdm

from .utils import dict_subset, unravel_dict, can_stringify, CONFIG, dt_to_str_in_dict

from os.path import dirname, abspath, join
from os import listdir

from .messages import *
from .utils import dirlock

# At the top of the file, after imports
if platform.system() == 'Windows':
    try:
        multiprocessing.set_start_method('spawn')
    except RuntimeError:
        # Context already set, that's okay
        pass

class Runner:

    def __init__(self, func, output, save_results=False, printlog=True, memory_limit=-1, log_level=logging.INFO):
        """
            Initialize the experiment runner

            :param func: the experiment function to call
            :param output: the root of the output directory
            :param save_results: whether to save results of experiments
            :param printlog: whether to print the log generated by the runner and or experiment to stdout
            :param memory_limit: the maximum amount of memory to allocate for this experiment in MB - ONLY WORKS ON LINUX!
            :param log_level: the logging level to use (see logging module for more info)
        """

        if save_results is False:
            print("Warning: save_results is False, results will not be saved")
        else:
            try:
                os.makedirs(output)
            except FileExistsError:
                answer = input(OUTPUT_DIR_EXISTS.format(output))
                if answer == "y":
                    pass
                else:
                    exit(1)
        
        self.func = func
        self.output_dir = output
        self.digits = 6
        self.memlimit = memory_limit
        global dirlock
        dirlock = multiprocessing.Lock()
        self.save_results = save_results

        if printlog is True:
            logging.basicConfig(level=log_level,
                                format="%(asctime)s [%(levelname)s] %(message)s",
                                handlers=[logging.StreamHandler(sys.stdout)])
        else:
            raise NotImplementedError("Logging to file not yet implemented")


    #####################################################
    #        Methods that should be overwritten         #
    #####################################################

    def make_kwargs(self, config):
        raise NotImplementedError(f"Implement `make_kwargs` for type {self}")

    def description(self, config) -> str:
        return f"Implement `description(config)` for {type(self)} to get informative progress"

    #####################################################
    #                   Main dispatch                   #
    #####################################################

    def run_one(self, config):
        """
            Run experiment with given configuration file.
            Config file will **not** be unraveled, and simply be passed to the experiment function
        """
        pool = multiprocessing.Pool(1, maxtasksperchild=1, initargs=(dirlock,))
        pool.map(self.run_experiment,[config])

    def run_batch(self, config, parallel=False, num_workers=None, show_progress=True):
        """
            Run a batch of experiments
            
            :param config: configuration dictionary
            :param parallel: whether to run experiments in parallel
            :param num_workers: number of worker processes (default: cpu_count - 1)
            :param show_progress: whether to show progress bar
        """
        configs = unravel_dict(config)
        total_exp = len(configs)

        if self.save_results:
            print("Filtering experiments based on disk")
            configs = self.filter_experiments(configs)
        print(f"Skipping {total_exp - len(configs)} experiments")
        self.n_experiments = len(configs)
        print(f"Running {len(configs)} remaining experiments")

        if parallel is True:
            if num_workers is None:
                num_workers = multiprocessing.cpu_count() - 1

            print(f"Running in parallel with {num_workers} processes")
            
            # Create processes that will each handle one experiment
            processes = []
            for config in configs:
                p = multiprocessing.Process(target=self.run_experiment, args=(config,))
                processes.append(p)
                p.start()
                
                # Wait if we've reached max workers
                while len([p for p in processes if p.is_alive()]) >= num_workers:
                    for p in processes:
                        if not p.is_alive():
                            p.join()
                            processes.remove(p)
                            break
                    time.sleep(1)  # Small delay to prevent CPU spinning
            
            # Wait for remaining processes
            for p in processes:
                p.join()

        else:
            if show_progress: pbar = tqdm(total=len(configs))
            for config in configs:
                if show_progress: pbar.set_description(self.description(config))
                self.run_experiment(config)
                if show_progress: pbar.update()

    def run_experiment(self, config):
        """
            Run a single experiment with the given configuration
        """
        if self.memlimit > 0:
            try:
                # Try Linux-specific approach first
                os_name = platform.platform()
                if os_name.startswith("Linux") and HAS_RESOURCE:
                    current_soft, hard = resource.getrlimit(resource.RLIMIT_AS)
                    limit_bytes = int(self.memlimit * 1024 * 1024)
                    print(f"Setting Linux memory limit to {limit_bytes} bytes")
                    resource.setrlimit(resource.RLIMIT_AS, (limit_bytes, hard))
                    use_linux_limit = True
                else:
                    use_linux_limit = False
                    raise ImportError("Not on Linux or resource module not available")
            except (ImportError, ValueError):
                # Fall back to psutil-based approach
                try:
                    import psutil
                    process = psutil.Process()
                    limit_bytes = int(self.memlimit * 1024 * 1024)
                    print(f"Setting memory limit to {limit_bytes} bytes using psutil")
                    use_linux_limit = False
                except ImportError:
                    print("Warning: psutil not installed. Memory limiting will be disabled.")
                    self.memlimit = -1
                    use_linux_limit = False

        if self.save_results:
            dirname = self.mkdir()

        kwargs = self.make_kwargs(config)
        try:
            if self.memlimit > 0:
                if not use_linux_limit:
                    # Monitor memory usage during execution
                    def check_memory():
                        if process.memory_info().rss > limit_bytes:
                            raise MemoryError("Memory limit exceeded")
                    
                    # Run the function with memory monitoring
                    import threading
                    monitor = threading.Thread(target=check_memory, daemon=True)
                    monitor.start()
                    result = self.func(**kwargs)
                    monitor.join(timeout=0.1)  # Stop monitoring
                else:
                    result = self.func(**kwargs)
            else:
                result = self.func(**kwargs)
        except MemoryError as e:
            result = dict(err=str(traceback.format_exc()))

        # Restore Linux memory limit if it was set
        if self.memlimit > 0 and use_linux_limit and HAS_RESOURCE:
            resource.setrlimit(resource.RLIMIT_AS, (current_soft, hard))

        if self.save_results:
            self.save_result(config, result, dirname)

    #####################################################
    #                   Helper functions                #
    #####################################################

    def remove_empty_subdirs(self):
        dirs =  listdir(self.output_dir)
        removed = 0
        pbar = tqdm(total=len(dirs))
        for edir in dirs:
            full_dir = join(self.output_dir, edir)
            if len(listdir(full_dir)) == 0:
                os.rmdir(full_dir)
                removed += 1
                pbar.set_description(f"Removed {removed} dirs")
            pbar.update()

        return True

    def filter_experiments(self, configs):
        """
            Filter experiments already finished in output directory
        """
        str_filtered = [dt_to_str_in_dict(c) for c in configs]
        filtered = copy.copy(configs)
        for edir in listdir(self.output_dir):
            full_dir = join(self.output_dir, edir)
            if os.path.isdir(full_dir):
                # robustify the code to some dummy files, e.g .DS_Store
                if CONFIG not in listdir(full_dir):
                    if len(listdir(full_dir)) != 0:
                        raise ValueError(f"{full_dir} is not emptpy, but does not contain {CONFIG}, was the directory created by RunExp?")
                    else:
                        continue
                with open(join(self.output_dir, edir, CONFIG), "r") as f:
                    disk_conf = json.loads(f.read())
                    if disk_conf in str_filtered:
                        idx = str_filtered.index(disk_conf)
                        filtered.pop(idx)
                        str_filtered.pop(idx)

        return filtered


    def mkdir(self):
        global dirlock
        with dirlock:
            idx = self.next_emtpy_index()
            # make dir for results
            dirname = (self.digits - len(str(idx))) * "0" + str(idx)
            full_dir = join(self.output_dir, dirname)
            try:
                os.mkdir(full_dir)
            except FileExistsError:
                print(len(full_dir))
                assert len(listdir(full_dir)) == 0, f"{full_dir} should be empty"
                pass
            return full_dir

    def next_emtpy_index(self):
        """
            RunExp creates output directories with numeric names.
            This function finds the next directory name
        """
        last_idx = 0
        for edir in sorted(listdir(self.output_dir)):
            if os.path.isdir(os.path.join(self.output_dir, edir)):
                # THis condition make the code robust to some files that your os can generate (like .DS_store..)
                idx = int(edir)
                if idx - 1 != last_idx:  # some missing number in the chain
                    return last_idx + 1
                # if len(listdir(join(self.output_dir, edir))) == 0:  # found empty dir
                #     return idx
                last_idx = idx
        return last_idx + 1

    def save_result(self, config, result, dirname):

        with open(join(dirname, CONFIG), "w") as f:
            f.write(json.dumps(dt_to_str_in_dict(config)))

        for key, value in result.items():
            if "." in str(key): # will assume extension is given
                with open(join(dirname, str(key)), "w") as f:
                    f.write(str(value))

            elif can_stringify(value):
                with open(join(dirname, str(key) + ".txt"), "w") as f:
                    f.write(str(value))

            elif isinstance(value, dict) and all(can_stringify(k) and can_stringify(v) for k,v in value.items()):
                with open(join(dirname, str(key) + ".json"), "w") as f:
                    f.write(json.dumps(value))

            elif isinstance(value, (list, set, tuple)) and all(can_stringify(v) for v in value):
                with open(join(dirname, str(key) + ".lst"), "w") as f:
                    f.write("\n".join([str(v) for v in value]))
            else:  # save as pickle
                with open(join(dirname, str(key) + ".pickle"), "wb") as f:
                    pickle.dump(value, f)
